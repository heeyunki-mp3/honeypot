{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrdJwlw2om7v"
   },
   "outputs": [],
   "source": [
    "#Import library\n",
    "import numpy as np\n",
    "import random as rm\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import sklearn as sc\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import csv\n",
    "from numpy.lib.shape_base import tile\n",
    "from sklearn.preprocessing import normalize\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import markov_clustering as mc\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import to_agraph\n",
    "import imageio\n",
    "from sklearn.metrics import silhouette_score\n",
    "import Levenshtein\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading cleaned data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_clean = pd.read_csv('../honeypot/cleaned_csv/pending_clean.csv')\n",
    "ebilling_clean = pd.read_csv('../honeypot/cleaned_csv/ebilling_clean.csv')\n",
    "csh_clean = pd.read_csv('../honeypot/cleaned_csv/csh_clean.csv')\n",
    "#pending_clean=pending_clean.drop([\t'Unnamed: 0',\t'Unnamed: 0.1',\t'Unnamed: 0.1.1'], axis=1)\n",
    "# ebilling_clean=ebilling_clean.drop([\t'Unnamed: 0',\t'Unnamed: 0.1',\t'Unnamed: 0.1.1'], axis=1)\n",
    "# csh_clean=csh_clean.drop([\t'Unnamed: 0',\t'Unnamed: 0.1',\t'Unnamed: 0.1.1'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_clean = pending_clean.drop(pending_clean[pending_clean['description'] == 'HTTP header'].index)\n",
    "ebilling_clean = ebilling_clean.drop(ebilling_clean[ebilling_clean['description'] == 'HTTP header'].index)\n",
    "csh_clean = csh_clean.drop(csh_clean[csh_clean['description'] == 'HTTP header'].index)\n",
    "pending_clean = pending_clean.drop(pending_clean[pending_clean['description'] == 'network scanner header'].index)\n",
    "ebilling_clean = ebilling_clean.drop(ebilling_clean[ebilling_clean['description'] == 'network scanner header'].index)\n",
    "csh_clean = csh_clean.drop(csh_clean[csh_clean['description'] == 'network scanner header'].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(pending_clean['session']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenshein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_char = \"!%&()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_abcdefghijklmnopqrstuvwxyz|~αβγδεζηθικλμνξοπρστυφχψωΘΔΣΩΠΛ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conversion = pd.read_csv('command_to_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv('./Bayes/data/testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_desc = set()\n",
    "# for stream in test_df['commands']:\n",
    "#     test_desc.update(set(eval(stream)))\n",
    "# test_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functionality_class_dict = dict()\n",
    "\n",
    "to_be_conv=list(set(df_conversion[\"Job\"]))\n",
    "\n",
    "for i in range(len(to_be_conv)):\n",
    "    functionality_class_dict[to_be_conv[i]] = possible_char[i]\n",
    "print(functionality_class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Honeypot:\n",
    "    def strip_session(self):\n",
    "        sessions = list(set(self.df['session']))\n",
    "        for sess in sessions: # new session (end of the one arrow)\n",
    "            df_sliced = self.df.loc[self.df[\"session\"] == sess][[\"description\"]]\n",
    "            df_sliced.index = range(len(df_sliced))\n",
    "            if len(df_sliced) == 1:\n",
    "                continue\n",
    "\n",
    "            list_long_desc = list(df_sliced['description'])\n",
    "            tmp_str_desc = \"\"\n",
    "            for des in list_long_desc:\n",
    "                tmp_str_desc += functionality_class_dict[des]\n",
    "\n",
    "            self.functionality_cluster_known.add(tmp_str_desc)\n",
    "            self.functionality_cluster_known_count_dict[tmp_str_desc] +=1\n",
    "\n",
    "\n",
    "            self.functionality_cluster_unknown.add(tmp_str_desc)\n",
    "            self.functionality_cluster_unknown_count_dict[tmp_str_desc] +=1\n",
    "            self.session_to_sequence[sess]=tmp_str_desc\n",
    "            self.sequence_to_session[tmp_str_desc].append(sess)\n",
    "\n",
    "        \n",
    "        # for i in range(len(test_df)):\n",
    "        #     stream = eval(test_df['commands'][i])\n",
    "        #     if len(stream) == 1:\n",
    "        #         continue\n",
    "            \n",
    "        #     tmp_str_desc = \"\"\n",
    "        #     for des in stream:\n",
    "        #         tmp_str_desc += functionality_class_dict[des]\n",
    "\n",
    "        #     self.functionality_cluster_known.add(tmp_str_desc)\n",
    "        #     self.functionality_cluster_known_count_dict[tmp_str_desc] +=1\n",
    "\n",
    "\n",
    "        #     self.functionality_cluster_unknown.add(tmp_str_desc)\n",
    "        #     self.functionality_cluster_unknown_count_dict[tmp_str_desc] +=1\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    def leven_dist_matrix(self, sequences):\n",
    "        num_sequences = len(sequences)\n",
    "        L_distance_matrix = np.zeros((num_sequences, num_sequences))\n",
    "\n",
    "        for i in range(num_sequences):\n",
    "            for j in range(i + 1, num_sequences):\n",
    "            \n",
    "                L_distance = Levenshtein.distance(sequences[i], sequences[j]) / (len(sequences[i]) + len(sequences[j]))\n",
    "                L_distance_matrix[i, j] = L_distance\n",
    "                L_distance_matrix[j, i] = L_distance\n",
    "        return L_distance_matrix\n",
    "    \n",
    "    def fk_leaf_label(self, i):\n",
    "        return self.functionality_cluster_known[i]\n",
    "\n",
    "\n",
    "    def __init__(self, df, name):\n",
    "        self.df = df\n",
    "        self.name =name\n",
    "        self.session_to_sequence = defaultdict(str)\n",
    "        self.functionality_cluster_known = set()\n",
    "        self.functionality_cluster_known_count_dict = defaultdict(int)\n",
    "        self.sequence_to_session = defaultdict(list)\n",
    "        self.functionality_cluster_unknown = set()\n",
    "        self.functionality_cluster_unknown_count_dict = defaultdict(int)\n",
    "\n",
    "        self.strip_session()\n",
    "       \n",
    "        self.functionality_cluster_known = list(self.functionality_cluster_known)\n",
    "        self.L_functionality_known =  self.leven_dist_matrix(self.functionality_cluster_known)\n",
    "        \n",
    "        self.functionality_cluster_unknown = list(self.functionality_cluster_unknown)\n",
    "        self.L_functionality_unknown =  self.leven_dist_matrix(self.functionality_cluster_unknown)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pending_clean.append(ebilling_clean, ignore_index=True)\n",
    "all_clean = temp.append(csh_clean, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_honey = Honeypot(pending_clean,\"pending\")\n",
    "ebilling_honey = Honeypot(ebilling_clean,\"ebilling\")\n",
    "csh_honey = Honeypot(csh_clean,\"csh\")\n",
    "all_honey = Honeypot(all_clean, \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_honey.sequence_to_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_honey.functionality_cluster_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clusters(distance_matrix, max_k):\n",
    "    silhouette_scores = []\n",
    "    for k in range(2, max_k + 1):\n",
    "        clustering = AgglomerativeClustering(n_clusters=k, affinity='precomputed', linkage='complete')\n",
    "        labels = clustering.fit_predict(distance_matrix)\n",
    "        score = silhouette_score(distance_matrix, labels, metric='precomputed')\n",
    "        silhouette_scores.append(score)\n",
    "    return silhouette_scores\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_silhouette_score(L_score, title):\n",
    "    # Evaluate clusters from 2 to 10 and plot\n",
    "    max_k = min(len(L_score) -1,30)\n",
    "    silhouette_scores = evaluate_clusters(L_score, max_k)\n",
    "\n",
    "    # Print the silhouette scores for different values of k\n",
    "    for k in range(2, max_k + 1):\n",
    "        print(f\"Silhouette score for {k} clusters: {silhouette_scores[k-2]}\")\n",
    "\n",
    "    # Plotting the silhouette scores\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(range(2, max_k + 1), silhouette_scores, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Score vs Number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_silhouette_score(pending_honey.L_functionality_known,\"[Pending] Functionality clutering known attacker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ebilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_silhouette_score(ebilling_honey.L_functionality_known,\"[Ebilling] Functionality clutering known attacker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_silhouette_score(all_honey.L_structure_known,\"[All] Structure clutering known attacker\")\n",
    "#graph_silhouette_score(all_honey.L_structure_unknown,\"[All] Structure clutering unknown ip\")\n",
    "graph_silhouette_score(all_honey.L_functionality_known,\"Silhouette Score\")\n",
    "#graph_silhouette_score(all_honey.L_functionality_unknown,\"[All] Functionality clutering unknown ip\")\n",
    "#graph_silhouette_score(all_honey.L_abstract_known,\"[All] Abstract clutering known attacker\")\n",
    "#graph_silhouette_score(all_honey.L_abstract_unknown,\"[All] Abstract clutering unknown ip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_silhouette_score(csh_honey.L_functionality_known,\"[Csh] Functionality clutering known attacker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Agglomerative Clustering: Selected the cluster size to be close to argmax(Silhouette score)\n",
    "def agglomerative_clustering(L_distance_matrix, n_clusters=15):\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters,  affinity='precomputed', linkage='complete') #Alternatively linkage could also be 'single'\n",
    "    labels = agg_clustering.fit_predict(L_distance_matrix)\n",
    "    #print(\"Cluster labels- Lavenshtein Agglomarative Clustering:\", labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglomerative_clustering_print(L_distance_matrix):\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=15,  affinity='precomputed', linkage='complete') #Alternatively linkage could also be 'single'\n",
    "    labels = agg_clustering.fit_predict(L_distance_matrix)\n",
    "    print(\"Cluster labels- Lavenshtein Agglomarative Clustering:\", labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_cmd_decoding(seq, dic):\n",
    "    conversion = dict((v,k) for k,v in dic.items())\n",
    "    ret = []\n",
    "    print(\"seq: \", seq)\n",
    "\n",
    "    for char in seq:\n",
    "        ret.append(conversion[char])\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_csv(name, row1, row2, conv_dict):\n",
    "    assert len(row1) == len(row2)\n",
    "\n",
    "    with open(name, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        field = ['sequence', 'label', 'command']\n",
    "        writer.writerow(field)\n",
    "        \n",
    "        for i in range(len(row1)):\n",
    "            seq = row1[i]\n",
    "           \n",
    "            # if \"_s\" in name:\n",
    "            #     tmp = [row1[i], row2[i]]\n",
    "\n",
    "            #     for i in str_cmd_decoding(str(seq),conv_dict):\n",
    "            #         tmp.append(label_to_ex_cmd[str(i)])\n",
    "\n",
    "            #     writer.writerow(tmp )\n",
    "\n",
    "            # else:\n",
    "            writer.writerow([row1[i], row2[i], str_cmd_decoding(str(seq),conv_dict)] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_csv_count(name, row1, row2, conv_dict, p_count, e_count, c_count, sequence_to_session):\n",
    "    assert len(row1) == len(row2)\n",
    "\n",
    "    with open(name, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        field = ['sequence', 'label', 'command', \"pending count\", \"ebilling count\" , \"csh count\", \"sessions\"]\n",
    "        writer.writerow(field)\n",
    "        print(p_count)\n",
    "        for i in range(len(row1)):\n",
    "            seq = str(row1[i])\n",
    "            print(seq)\n",
    "            writer.writerow([row1[i], row2[i], str_cmd_decoding(seq,conv_dict), p_count[seq], e_count[seq], c_count[seq], sequence_to_session[seq] ] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_clustering(honey, name, k=15):\n",
    "    if name == \"TEST_all_normalized_IP\":\n",
    "        labels = agglomerative_clustering(honey.L_functionality_known,k)\n",
    "\n",
    "        dump_csv_count(f'{name}_fk_ipip.csv',\n",
    "                 honey.functionality_cluster_known,\n",
    "                 labels,\n",
    "                 functionality_class_dict,\n",
    "                 pending_honey.functionality_cluster_known_count_dict,\n",
    "                 ebilling_honey.functionality_cluster_known_count_dict,\n",
    "                 csh_honey.functionality_cluster_known_count_dict, honey.sequence_to_session)\n",
    "    else:\n",
    "        labels = agglomerative_clustering(honey.L_functionality_unknown,k)\n",
    "        dump_csv(f'{name}_fu.csv',honey.functionality_cluster_unknown, labels, functionality_class_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_honey.functionality_cluster_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_clustering(all_honey, \"TEST_all_normalized_IP\",14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_honey.sequence_to_session[\"TL?F9KYfg&<Ag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_clustering(pending_honey, \"pending_normalized\")\n",
    "# dump_clustering(ebilling_honey, \"ebilling_normalized\")\n",
    "dump_clustering(csh_honey, \"TEST_csh_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files\n",
    "file_with_sessions = \"./TEST_all_normalized_IP_fk_ipip.csv\"  # The file that has 'sessions'\n",
    "file_without_sessions = \"../Bayes/data/training_data.csv\"  # The file that needs 'sessions'\n",
    "\n",
    "df1 = pd.read_csv(file_with_sessions)  # Contains 'command' and 'sessions'\n",
    "df2 = pd.read_csv(file_without_sessions)  # Contains 'command' but no 'sessions'\n",
    "\n",
    "# Merge based on 'command' while preserving all rows in df2\n",
    "df_merged = df2.merge(df1[['command', 'sessions']], on='command', how='left')\n",
    "\n",
    "# Save the updated file\n",
    "output_file = \"updated_file.csv\"\n",
    "df_merged.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IP address rating\n",
    "\n",
    "#List of file paths\n",
    "file_paths = [\"ip_report_crm.csv\", \"ip_report_ebilling.csv\", \"ip_report_pending.csv\"]  # Replace with actual file names\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "session_dict = {}\n",
    "\n",
    "# Read each file and update the dictionary\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "    for _, row in df.iterrows():\n",
    "        session_dict[row['session']] = {\n",
    "            'clean': row['clean'],\n",
    "            'unrated': row['unrated'],\n",
    "            'suspicious': row['suspicious'],\n",
    "            'malicious': row['malicious']\n",
    "        }\n",
    "\n",
    "# Print or use the combined dictionary\n",
    "print(session_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_test = [\"ip_report_crm_testing.csv\", \"ip_report_ebilling_testing.csv\", \"ip_report_pending_testing.csv\"]  # Replace with actual file names\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "session_dict_test = {}\n",
    "\n",
    "# Read each file and update the dictionary\n",
    "for file in file_paths_test:\n",
    "    df = pd.read_csv(file)\n",
    "    for _, row in df.iterrows():\n",
    "        session_dict_test[row['session']] = {\n",
    "            'clean': row['clean'],\n",
    "            'unrated': row['unrated'],\n",
    "            'suspicious': row['suspicious'],\n",
    "            'malicious': row['malicious']\n",
    "        }\n",
    "\n",
    "# Print or use the combined dictionary\n",
    "print(session_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_honeypot_rate(file):\n",
    "# Initialize an empty dictionary\n",
    "    session_dict = {}\n",
    "\n",
    "    # Read each file and update the dictionary\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    for _, row in df.iterrows():\n",
    "        session_dict[row['session']] = {\n",
    "            'clean': row['clean'],\n",
    "            'unrated': row['unrated'],\n",
    "            'suspicious': row['suspicious'],\n",
    "            'malicious': row['malicious']\n",
    "        }\n",
    "    return df, session_dict\n",
    "\n",
    "    # Print or use the combined dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rate_pending = df_honeypot_rate(\"ip_report_pending.csv\")\n",
    "# df_rate_ebilling = df_honeypot_rate(\"ip_report_ebilling.csv\")\n",
    "# df_rate_crm = df_honeypot_rate(\"ip_report_crm.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rate_pending.to_csv(\"pending_rating.csv\")\n",
    "# df_rate_ebilling.to_csv(\"ebilling_rating.csv\")\n",
    "# df_rate_crm.to_csv(\"crm_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_merged\n",
    "\n",
    "# Ensure 'sessions' column is a list (assuming it’s stored as a string of lists in CSV)\n",
    "df['sessions'] = df['sessions'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(', ') if isinstance(x, str) else [])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../Bayes/data/ground_truth.csv\")\n",
    "\n",
    "# Ensure 'sessions' column is a list (assuming it’s stored as a string of lists in CSV)\n",
    "df_test['sessions'] = df_test['sessions'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(', ') if isinstance(x, str) else [])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk_score(sessions,session_dictionary,t):\n",
    "    risk_scores = []\n",
    "    for session in sessions:\n",
    "\n",
    "        if session in session_dictionary:\n",
    "            data = session_dictionary[session]\n",
    "\n",
    "            # print((data['clean'] +data['suspicious']+data['malicious']))\n",
    "            \n",
    "            if (data['clean'] +data['suspicious']+data['malicious']) >0:\n",
    "                if data['malicious'] >t:\n",
    "                    risk_score = 1\n",
    "                elif data['suspicious'] + data['malicious']>t:\n",
    "                    risk_score = 0.5\n",
    "                else:\n",
    "                    risk_score = 0\n",
    "\n",
    "                risk_scores.append(risk_score)\n",
    "\n",
    "    final_risk_score = sum(risk_scores) / len(risk_scores) if risk_scores else 0\n",
    "    \n",
    "    return final_risk_score, len(risk_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate risk score\n",
    "def calculate_risk_score(session,session_dictionary,t):\n",
    "    risk_scores = []\n",
    "\n",
    "\n",
    "    if session in session_dictionary:\n",
    "        data = session_dictionary[session]\n",
    "\n",
    "        # print((data['clean'] +data['suspicious']+data['malicious']))\n",
    "        \n",
    "        if (data['clean'] +data['suspicious']+data['malicious']) >0:\n",
    "            if data['malicious'] >t:\n",
    "                risk_score = 1\n",
    "            elif data['suspicious'] + data['malicious']>t:\n",
    "                risk_score = 0.5\n",
    "            else:\n",
    "                risk_score = 0\n",
    "            #risk_score = (data['clean'] * 0 + (data['suspicious'] * 0.5) + (data['malicious'] * 1)) / (data['clean'] +data['suspicious']+data['malicious'])\n",
    "            # print(data['clean'],data['suspicious'],data['malicious'],\":\", risk_score)\n",
    "            # if risk_score != 0:\n",
    "            #     risk_score = math.log10(risk_score) * 0.5 + 1\n",
    "            # if risk_score < 0:\n",
    "            #     risk_score = 0\n",
    "            risk_scores.append(risk_score)\n",
    "\n",
    "    final_risk_score = sum(risk_scores) / len(risk_scores) if risk_scores else 0\n",
    "    \n",
    "    return final_risk_score, len(risk_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Safely convert sessions to lists if they are strings\n",
    "def safe_literal_eval(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return [x]\n",
    "    return x\n",
    "\n",
    "# Apply the safe conversion\n",
    "df['sessions'] = df['sessions'].apply(safe_literal_eval)\n",
    "\n",
    "# Explode to flatten the sessions\n",
    "df_flattened = df.explode('sessions').reset_index(drop=True)\n",
    "\n",
    "# Check result\n",
    "df_flattened.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_flattened[\"sessions\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df_flattened[['risk_score', 'risk_score_count']] = df_flattened['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,session_dict,1)))\n",
    "df_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to DataFrame and store both risk score and count\n",
    "\n",
    "for t in range(10):\n",
    "    print(f\"======================threshold GREATER than {t}======================\")\n",
    "    df[['risk_score', 'risk_score_count']] = df['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,session_dict,t)))\n",
    "\n",
    "    for i in [0,1,3,5,7,8,9]:\n",
    "        temp = df.loc[df['label'] == i]\n",
    "        #print(temp['risk_score'])\n",
    "        weighted_average = sum(temp['risk_score'] * temp['risk_score_count']) / sum(temp['risk_score_count'])\n",
    "        print(i, weighted_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_threshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['risk_score', 'risk_score_count']] = df['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,session_dict,agreement_threshold)))\n",
    "train_calc_dict = dict()\n",
    "\n",
    "for i in [0,1,3,5,7,8,9]:\n",
    "    temp = df.loc[df['label'] == i]\n",
    "    #print(temp['risk_score'])\n",
    "    weighted_average = sum(temp['risk_score'] * temp['risk_score_count']) / sum(temp['risk_score_count'])\n",
    "    print(i, weighted_average, sum(temp['risk_score_count']))\n",
    "    \n",
    "    train_calc_dict[i] = [weighted_average, sum(temp['risk_score_count'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['risk_score', 'risk_score_count']] = df_test['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,session_dict_test,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['risk_score', 'risk_score_count']] = df_test['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,session_dict_test,agreement_threshold)))\n",
    "df_test.dropna()\n",
    "df_test.index = range(len(df_test))\n",
    "\n",
    "test_calc_dict = dict()\n",
    "\n",
    "for i in [0,1,3,5,7,8,9]:\n",
    "    if i in list(df_test['ground_truth']):\n",
    "\n",
    "        temp = df_test.loc[df_test['ground_truth'] == i]\n",
    "\n",
    "        #print(temp['risk_score'])\n",
    "        weighted_average = sum(temp['risk_score'] * temp['risk_score_count']) / sum(temp['risk_score_count'])\n",
    "        print(i, weighted_average, sum(temp['risk_score_count']))\n",
    "        test_calc_dict[i] = [weighted_average, sum(temp['risk_score_count'])]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10):\n",
    "    print(f\"======================threshold GREATER than {t}======================\")\n",
    "    \n",
    "    df[['risk_score', 'risk_score_count']] = df['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,session_dict,t)))\n",
    "    train_calc_dict = dict()\n",
    "\n",
    "    for i in [0,1,3,5,7,8,9]:\n",
    "        temp = df.loc[df['label'] == i]\n",
    "        #print(temp['risk_score'])\n",
    "        weighted_average = sum(temp['risk_score'] * temp['risk_score_count']) / sum(temp['risk_score_count'])\n",
    "        # print(i, weighted_average, sum(temp['risk_score_count']))\n",
    "        \n",
    "        train_calc_dict[i] = [weighted_average, sum(temp['risk_score_count'])]\n",
    "        \n",
    "    df_test[['risk_score', 'risk_score_count']] = df_test['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,session_dict_test,t)))\n",
    "    df_test.dropna()\n",
    "    df_test.index = range(len(df_test))\n",
    "\n",
    "    test_calc_dict = dict()\n",
    "\n",
    "    for i in [0,1,3,5,7,8,9]:\n",
    "        if i in list(df_test['ground_truth']):\n",
    "\n",
    "            temp = df_test.loc[df_test['ground_truth'] == i]\n",
    "\n",
    "            #print(temp['risk_score'])\n",
    "            weighted_average = sum(temp['risk_score'] * temp['risk_score_count']) / sum(temp['risk_score_count'])\n",
    "            # print(i, weighted_average, sum(temp['risk_score_count']))\n",
    "            test_calc_dict[i] = [weighted_average, sum(temp['risk_score_count'])]\n",
    "        else:\n",
    "            test_calc_dict[i] = [0,0]\n",
    "            \n",
    "    for i in [0,1,3,5,7,8,9]:\n",
    "        score = (train_calc_dict[i][0] *  train_calc_dict[i][1] + test_calc_dict[i][0] *  test_calc_dict[i][1] )/ (train_calc_dict[i][1] +test_calc_dict[i][1])\n",
    "        print(i, score, train_calc_dict[i][1], test_calc_dict[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def honeypot_rate_cluster(file,t):\n",
    "    _, session_dict = df_honeypot_rate(file) \n",
    "    # Load the DataFrame\n",
    "    df=df_merged\n",
    "\n",
    "    # Ensure 'sessions' column is a list (assuming it’s stored as a string of lists in CSV)\n",
    "    df['sessions'] = df['sessions'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(', ') if isinstance(x, str) else [])\n",
    "\n",
    "    # Function to calculate risk score\n",
    "    def calculate_risk_score(sessions):\n",
    "        risk_scores = []\n",
    "        for session in sessions:\n",
    "            if session in session_dict:\n",
    "                data = session_dict[session]\n",
    "                # print((data['clean'] +data['suspicious']+data['malicious']))\n",
    "                if (data['clean'] +data['suspicious']+data['malicious']) >0:\n",
    "                    if data['malicious'] >t:\n",
    "                        risk_score = 1\n",
    "                    elif data['suspicious'] + data['malicious']>t:\n",
    "                        risk_score = 0.5\n",
    "                    else:\n",
    "                        risk_score = 0  \n",
    "                # if (data['clean'] +data['suspicious']+data['malicious']) >0:\n",
    "                #     risk_score = (data['clean'] * 0 + (data['suspicious'] * 0.5) + (data['malicious'] * 1)) / (data['clean'] +data['suspicious']+data['malicious'])\n",
    "                #     # print(data['clean'],data['suspicious'],data['malicious'],\":\", risk_score)\n",
    "                #     # if risk_score != 0:\n",
    "                #     #     risk_score = math.log10(risk_score) * 0.5 + 1\n",
    "                #     # if risk_score < 0:\n",
    "                #     #     risk_score = 0\n",
    "                    risk_scores.append(risk_score)\n",
    "\n",
    "        final_risk_score = sum(risk_scores) / len(risk_scores) if risk_scores else 0\n",
    "\n",
    "        return final_risk_score, len(risk_scores)\n",
    "\n",
    "    # Apply function to DataFrame and store both risk score and count\n",
    "    df[['risk_score', 'risk_score_count']] = df['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x)))\n",
    "\n",
    "    # Display DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def honeypot_rate_cluster(file):\n",
    "    _, session_dict = df_honeypot_rate(file) \n",
    "    # Load the DataFrame\n",
    "    # df=df_merged\n",
    "\n",
    "    # Ensure 'sessions' column is a list (assuming it’s stored as a string of lists in CSV)\n",
    "    # df['sessions'] = df['sessions'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(', ') if isinstance(x, str) else [])\n",
    "    df= pd.read_csv(file)\n",
    "    def categorize_row(row):\n",
    "        if row['malicious'] >= 2:\n",
    "            return 'malicious'\n",
    "        elif (row['malicious'] + row['suspicious']) >= 2:\n",
    "            return 'suspicious'\n",
    "        else:\n",
    "            return 'clean'\n",
    "\n",
    "    # Apply categorization function to each row and add results as a new column\n",
    "    df['category'] = df.apply(categorize_row, axis=1)\n",
    "\n",
    "    # Aggregate counts of each category\n",
    "    category_counts = df['category'].value_counts().reset_index()\n",
    "    category_counts.columns = ['category', 'count']\n",
    "\n",
    "    # Output aggregated data to CSV\n",
    "    category_counts.to_csv(f'aggregated_categories_{file}', index=False)\n",
    "\n",
    "    # (Optional) print the output to verify\n",
    "    print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honeypot_rate_cluster(\"ip_report_pending.csv\")\n",
    "honeypot_rate_cluster(\"ip_report_ebilling.csv\")\n",
    "honeypot_rate_cluster(\"ip_report_crm.csv\")\n",
    "honeypot_rate_cluster(\"ip_report_pending_testing.csv\")\n",
    "honeypot_rate_cluster(\"ip_report_ebilling_testing.csv\")\n",
    "honeypot_rate_cluster(\"ip_report_crm_testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def honeypot_rate_cluster(file):\n",
    "    _, session_dict = df_honeypot_rate(file) \n",
    "    # Load the DataFrame\n",
    "    df=df_merged\n",
    "\n",
    "    # Ensure 'sessions' column is a list (assuming it’s stored as a string of lists in CSV)\n",
    "    df['sessions'] = df['sessions'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(', ') if isinstance(x, str) else [])\n",
    "\n",
    "    # Function to calculate risk score\n",
    "    def calculate_risk_score(sessions, rating):\n",
    "        risk_scores = []\n",
    "        for session in sessions:\n",
    "            if session in session_dict:\n",
    "                data = session_dict[session]\n",
    "                risk_scores.append(data[rating])\n",
    "                     \n",
    "        final_risk_score = sum(risk_scores) \n",
    "\n",
    "        return final_risk_score, len(risk_scores)\n",
    "\n",
    "    # Apply function to DataFrame and store both risk score and count\n",
    "    df[['clean', '0']] = df['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,\"clean\")))\n",
    "    df[['unrated', '1']] = df['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,\"unrated\")))\n",
    "    df[['suspicious', '2']] = df['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,\"suspicious\")))\n",
    "    df[['malicious', '3']] = df['sessions'].apply(lambda x: pd.Series(calculate_risk_score(x,\"malicious\")))\n",
    "\n",
    "    # Display DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two CSV files\n",
    "file_with_sessions = \"./TEST_all_normalized_IP_fk_ipip.csv\"  # The file that has 'sessions'\n",
    "file_without_sessions = \"../Bayes/data/training_data.csv\"  # The file that needs 'sessions'\n",
    "\n",
    "df1 = pd.read_csv(file_with_sessions)  # Contains 'command' and 'sessions'\n",
    "df2 = pd.read_csv(file_without_sessions)  # Contains 'command' but no 'sessions'\n",
    "\n",
    "# Merge based on 'command' while preserving all rows in df2\n",
    "df_merged = df2.merge(df1[['command', 'sessions']], on='command', how='left')\n",
    "\n",
    "# Save the updated file\n",
    "output_file = \"updated_file.csv\"\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "df_pending_rated = honeypot_rate_cluster(\"ip_report_pending.csv\")\n",
    "del df_pending_rated['sessions']\n",
    "df_pending_rated.to_csv(\"df_pending_rated_01.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the two CSV files\n",
    "file_with_sessions = \"./TEST_all_normalized_IP_fk_ipip.csv\"  # The file that has 'sessions'\n",
    "file_without_sessions = \"../Bayes/data/training_data.csv\"  # The file that needs 'sessions'\n",
    "\n",
    "df1 = pd.read_csv(file_with_sessions)  # Contains 'command' and 'sessions'\n",
    "df2 = pd.read_csv(file_without_sessions)  # Contains 'command' but no 'sessions'\n",
    "\n",
    "# Merge based on 'command' while preserving all rows in df2\n",
    "df_merged = df2.merge(df1[['command', 'sessions']], on='command', how='left')\n",
    "\n",
    "# Save the updated file\n",
    "output_file = \"updated_file.csv\"\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "df_ebilling_rated = honeypot_rate_cluster(\"ip_report_ebilling.csv\")\n",
    "del df_ebilling_rated['sessions']\n",
    "df_ebilling_rated.to_csv(\"df_ebilling_rated_01.csv\")\n",
    "\n",
    "\n",
    "# Load the two CSV files\n",
    "file_with_sessions = \"./TEST_all_normalized_IP_fk_ipip.csv\"  # The file that has 'sessions'\n",
    "file_without_sessions = \"../Bayes/data/training_data.csv\"  # The file that needs 'sessions'\n",
    "\n",
    "df1 = pd.read_csv(file_with_sessions)  # Contains 'command' and 'sessions'\n",
    "df2 = pd.read_csv(file_without_sessions)  # Contains 'command' but no 'sessions'\n",
    "\n",
    "# Merge based on 'command' while preserving all rows in df2\n",
    "df_merged = df2.merge(df1[['command', 'sessions']], on='command', how='left')\n",
    "\n",
    "# Save the updated file\n",
    "output_file = \"updated_file.csv\"\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "df_crm_rated = honeypot_rate_cluster(\"ip_report_crm.csv\")\n",
    "del df_crm_rated['sessions']\n",
    "df_crm_rated.to_csv(\"df_crm_rated_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list to store individual session risk scores\n",
    "expanded_data = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "\n",
    "    label = row['label']  # Extract label\n",
    "    sessions = row['sessions']  # Extract list of sessions\n",
    "\n",
    "    for session in sessions:\n",
    "        if session in session_dict:\n",
    "            data = session_dict[session]\n",
    "            total_classifications = data['clean'] + data['suspicious'] + data['malicious']\n",
    "\n",
    "            if total_classifications > 0:\n",
    "                risk_score = (data['suspicious'] * 0.5 + data['malicious'] * 1) / total_classifications\n",
    "                \n",
    "                # Append to the new dataset (individual risk score per session)\n",
    "                expanded_data.append({'label': label, 'session': session, 'risk_score': risk_score})\n",
    "\n",
    "# Convert to a DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_data)\n",
    "\n",
    "# Save to CSV\n",
    "expanded_df.to_csv(\"session_risk_scores_again.csv\", index=False)\n",
    "\n",
    "# # Display the new DataFrame\n",
    "# import ace_tools as tools\n",
    "# tools.display_dataframe_to_user(name=\"Session Risk Scores CSV\", dataframe=expanded_df)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
